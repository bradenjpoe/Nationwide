{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Report & Data Enhancement\n",
    "### Braden Poe | Nationwide Work Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Using Las Vegas restaurant inspection data we want to predict the probability of a restaurant receiving a critical inspection. The current trimmed dataset identifies restaurants by city, zipcode, lagged inspection data, and a binary response variable. Roughly 15,000 observations are available through which the model can be constructed. Two models were seriously considered in this portion of the analysis: Random Forest, and Logistic Regression. \n",
    "\n",
    "I ruled out logistic regression due to the inseparability of the data and high dimensionality of the dataset. Speaking to inseparability, I felt that any type of linear boundary would do a poor job at classification. This concern could be addressed with a non-linear boundary, but the already high dimensionality of the dataset would lead to a massive polynomial in the loss function. I could have lessened dimensionality through feature selection, but that was not the most efficient option with other techniques on the table. \n",
    "\n",
    "Random forest seemed like the most effective method for dealing with inseparability; given the nice distributional spread across \"lagged demerits\" and \"lagged violations\" I am confident that no extreme outliers exist. Estimation can be poor for rare observations if the dataset is not representative of the population, but since the data featured every Las Vegas restaurant, that was not a concern. Further, the ability to tune hyperparameters was a major advantage. With logistic regression I would have implemented an L1 penalty to more aggressively eliminate features, but that is the only hyperparameter I could have tuned. Random forest classification opens up the possibility for significant model adjustments that can effectively mimic the non-linearity required by the data.\n",
    "\n",
    "As a final consideration, I briefly looked at implementing a neural net, but that seemed like overkill given the sample size. If I was working with a national dataset or even a statewide dataset, then a neural net may have been a more logical choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data & Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I want to import the final csv from the previous report and then establish a naive baseline to which I can compare my trained model. The naive baseline will simply be predicting one every time, since 'Response' = 1 is the majority of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The naive prediction accuracy when predicting response equal to 1 is the mean of our response variable.\n",
      "\n",
      "Naive Accuracy: 0.5768\n"
     ]
    }
   ],
   "source": [
    "# Import model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Import data\n",
    "data = pd.read_csv('predict.csv',index_col=None)\n",
    "data = data.drop('Unnamed: 0',axis=1)\n",
    "data.insert(0,'index',range(0,len(data)))\n",
    "data = data.set_index('index')\n",
    "\n",
    "# Set X and y data \n",
    "y = data['response']\n",
    "X = data.drop('response',axis=1)\n",
    "col_names  = X.columns\n",
    "col = np.array(col_names).reshape(127,1)\n",
    "print('The naive prediction accuracy when predicting response equal to 1 is the mean of our response variable.')\n",
    "print('\\nNaive Accuracy: {0}'.format(round(y.mean(),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring precision and recall for the moment, if the trained model accuracy cannot beat the naive prediction accuracy above then there is no way to justify implementation. I hope to see accuracy over 90%, but am keeping in mind that many demographic variables are missing from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Baseline without Feature Selection, Tuning, or CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted     0     1\n",
      "Actual               \n",
      "0          1021   998\n",
      "1           821  1881\n",
      "\n",
      "The model precision is: 0.653\n",
      "The model recall is: 0.696\n",
      "The model accuracy is: 0.615\n",
      "\n",
      "\n",
      "Took 44.27 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import warnings\n",
    "import timeit\n",
    "\n",
    "t0 = timeit.default_timer()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Train and test splits\n",
    "X_train, X_test, y_train, y_test = tts(X,y, test_size=.3, random_state= 1)\n",
    "\n",
    "# Let's run a straight forward estimate to see how the model generally does w/out tuning or CV\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000, max_depth = 50,random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "t20 = feature_importances['importance'].head(20)\n",
    "pred = rf.predict(X_test)\n",
    "pred[pred >= 0.5] = 1\n",
    "pred[pred < 0.5] = 0\n",
    "pred = pred.astype(int)\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = pd.crosstab(y_test,pred,rownames=['Actual'],colnames=['Predicted'])\n",
    "print(cm)\n",
    "\n",
    "# Accuracy, Recall, Precision\n",
    "pre = cm.loc[1,1]/(cm.loc[0,1]+cm.loc[1,1])\n",
    "print('\\nThe model precision is: {0}'.format(round(pre,3)))\n",
    "rec = cm.loc[1,1]/(cm.loc[1,0]+cm.loc[1,1])\n",
    "print('The model recall is: {0}'.format(round(rec,3)))\n",
    "acc = (cm.loc[1,1]+cm.loc[0,0])/(cm.loc[1,1]+cm.loc[0,0] + cm.loc[0,1] + cm.loc[1,0])\n",
    "print('The model accuracy is: {0}'.format(round(acc,3)))\n",
    "t1 = timeit.default_timer()\n",
    "total = t1 - t0\n",
    "print('\\n\\nTook {0} seconds to run.'.format(round(total,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model with no tuning is only 4% more accurate than the naive baseline at 62% with precision and recall at roughly 70%. The early returns are not ideal - I would have liked to see an accuracy score in the mid 70's with recall correspondingly high since we would be worried about false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am taking the 20 most important features from the previously trained model and will manually narrow that list down further to gain an optimal number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted     0     1\n",
      "Actual               \n",
      "0          1021   998\n",
      "1           733  1969\n",
      "\n",
      "The model precision is: 0.664\n",
      "The model recall is: 0.729\n",
      "The model accuracy is: 0.633\n",
      "\n",
      "\n",
      "Took 17.11 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "# Adjust X and y to reflect top 20 features\n",
    "cols_to_keep = list(t20.index)\n",
    "X = X[cols_to_keep]\n",
    "y = y\n",
    "\n",
    "# New Baseline\n",
    "t0 = timeit.default_timer()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Train and test splits\n",
    "X_train, X_test, y_train, y_test = tts(X,y, test_size=.3, random_state= 1)\n",
    "\n",
    "# Let's run a straight forward estimate to see how the model generally does w/out tuning or CV\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000, max_depth = 50,random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "pred = rf.predict(X_test)\n",
    "pred[pred >= 0.5] = 1\n",
    "pred[pred < 0.5] = 0\n",
    "pred = pred.astype(int)\n",
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "t10 = feature_importances['importance'].head(10)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = pd.crosstab(y_test,pred,rownames=['Actual'],colnames=['Predicted'])\n",
    "print(cm)\n",
    "\n",
    "# Accuracy, Recall, Precision\n",
    "pre = cm.loc[1,1]/(cm.loc[0,1]+cm.loc[1,1])\n",
    "print('\\nThe model precision is: {0}'.format(round(pre,3)))\n",
    "rec = cm.loc[1,1]/(cm.loc[1,0]+cm.loc[1,1])\n",
    "print('The model recall is: {0}'.format(round(rec,3)))\n",
    "acc = (cm.loc[1,1]+cm.loc[0,0])/(cm.loc[1,1]+cm.loc[0,0] + cm.loc[0,1] + cm.loc[1,0])\n",
    "print('The model accuracy is: {0}'.format(round(acc,3)))\n",
    "t1 = timeit.default_timer()\n",
    "total = t1 - t0\n",
    "print('\\n\\nTook {0} seconds to run.'.format(round(total,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy and precision improved slightly with narrower features. I will consider narrowing to 10 features to see if results change at all. In the interest of computation time, the relevant features should be minimized while maximizing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted     0     1\n",
      "Actual               \n",
      "0          1049   970\n",
      "1           736  1966\n",
      "\n",
      "The model precision is: 0.67\n",
      "The model recall is: 0.728\n",
      "The model accuracy is: 0.639\n",
      "\n",
      "\n",
      "Took 12.81 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "# Adjust X and y to reflect top 10 features\n",
    "cols_to_keep = list(t10.index)\n",
    "X = X[cols_to_keep]\n",
    "y = y\n",
    "\n",
    "# New Baseline\n",
    "t0 = timeit.default_timer()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Train and test splits\n",
    "X_train, X_test, y_train, y_test = tts(X,y, test_size=.3, random_state= 1)\n",
    "\n",
    "# Let's run a straight forward estimate to see how the model generally does w/out tuning or CV\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000, max_depth = 50,random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "pred = rf.predict(X_test)\n",
    "pred[pred >= 0.5] = 1\n",
    "pred[pred < 0.5] = 0\n",
    "pred = pred.astype(int)\n",
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "t5 = feature_importances['importance'].head(5)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = pd.crosstab(y_test,pred,rownames=['Actual'],colnames=['Predicted'])\n",
    "print(cm)\n",
    "\n",
    "# Accuracy, Recall, Precision\n",
    "pre = cm.loc[1,1]/(cm.loc[0,1]+cm.loc[1,1])\n",
    "print('\\nThe model precision is: {0}'.format(round(pre,3)))\n",
    "rec = cm.loc[1,1]/(cm.loc[1,0]+cm.loc[1,1])\n",
    "print('The model recall is: {0}'.format(round(rec,3)))\n",
    "acc = (cm.loc[1,1]+cm.loc[0,0])/(cm.loc[1,1]+cm.loc[0,0] + cm.loc[0,1] + cm.loc[1,0])\n",
    "print('The model accuracy is: {0}'.format(round(acc,3)))\n",
    "t1 = timeit.default_timer()\n",
    "total = t1 - t0\n",
    "print('\\n\\nTook {0} seconds to run.'.format(round(total,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted     0     1\n",
      "Actual               \n",
      "0          1011  1008\n",
      "1           666  2036\n",
      "\n",
      "The model precision is: 0.669\n",
      "The model recall is: 0.754\n",
      "The model accuracy is: 0.645\n",
      "\n",
      "\n",
      "Took 12.76 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "# Adjust X and y to reflect top 5 features\n",
    "cols_to_keep = list(t5.index)\n",
    "X = X[cols_to_keep]\n",
    "y = y\n",
    "\n",
    "# New Baseline\n",
    "t0 = timeit.default_timer()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Train and test splits\n",
    "X_train, X_test, y_train, y_test = tts(X,y, test_size=.3, random_state= 1)\n",
    "\n",
    "# Let's run a straight forward estimate to see how the model generally does w/out tuning or CV\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000, max_depth = 50,random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "pred = rf.predict(X_test)\n",
    "pred[pred >= 0.5] = 1\n",
    "pred[pred < 0.5] = 0\n",
    "pred = pred.astype(int)\n",
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "t4 = feature_importances['importance'].head(4)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = pd.crosstab(y_test,pred,rownames=['Actual'],colnames=['Predicted'])\n",
    "print(cm)\n",
    "\n",
    "# Accuracy, Recall, Precision\n",
    "pre = cm.loc[1,1]/(cm.loc[0,1]+cm.loc[1,1])\n",
    "print('\\nThe model precision is: {0}'.format(round(pre,3)))\n",
    "rec = cm.loc[1,1]/(cm.loc[1,0]+cm.loc[1,1])\n",
    "print('The model recall is: {0}'.format(round(rec,3)))\n",
    "acc = (cm.loc[1,1]+cm.loc[0,0])/(cm.loc[1,1]+cm.loc[0,0] + cm.loc[0,1] + cm.loc[1,0])\n",
    "print('The model accuracy is: {0}'.format(round(acc,3)))\n",
    "t1 = timeit.default_timer()\n",
    "total = t1 - t0\n",
    "print('\\n\\nTook {0} seconds to run.'.format(round(total,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    0     1\n",
      "Actual              \n",
      "0          958  1061\n",
      "1          648  2054\n",
      "\n",
      "The model precision is: 0.659\n",
      "The model recall is: 0.76\n",
      "The model accuracy is: 0.638\n",
      "\n",
      "\n",
      "Took 9.36 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "# Adjust X and y to reflect top 4 features\n",
    "cols_to_keep = list(t4.index)\n",
    "X = X[cols_to_keep]\n",
    "y = y\n",
    "\n",
    "# New Baseline\n",
    "t0 = timeit.default_timer()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Train and test splits\n",
    "X_train, X_test, y_train, y_test = tts(X,y, test_size=.3, random_state= 1)\n",
    "\n",
    "# Let's run a straight forward estimate to see how the model generally does w/out tuning or CV\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000, max_depth = 50,random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "pred = rf.predict(X_test)\n",
    "pred[pred >= 0.5] = 1\n",
    "pred[pred < 0.5] = 0\n",
    "pred = pred.astype(int)\n",
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "t4 = feature_importances['importance'].head(4)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = pd.crosstab(y_test,pred,rownames=['Actual'],colnames=['Predicted'])\n",
    "print(cm)\n",
    "\n",
    "# Accuracy, Recall, Precision\n",
    "pre = cm.loc[1,1]/(cm.loc[0,1]+cm.loc[1,1])\n",
    "print('\\nThe model precision is: {0}'.format(round(pre,3)))\n",
    "rec = cm.loc[1,1]/(cm.loc[1,0]+cm.loc[1,1])\n",
    "print('The model recall is: {0}'.format(round(rec,3)))\n",
    "acc = (cm.loc[1,1]+cm.loc[0,0])/(cm.loc[1,1]+cm.loc[0,0] + cm.loc[0,1] + cm.loc[1,0])\n",
    "print('The model accuracy is: {0}'.format(round(acc,3)))\n",
    "t1 = timeit.default_timer()\n",
    "total = t1 - t0\n",
    "print('\\n\\nTook {0} seconds to run.'.format(round(total,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "demerits_lag    0.370284\n",
       "nm_lag          0.256711\n",
       "numvio_lag      0.211941\n",
       "crit_lag        0.161063\n",
       "Name: importance, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process to determine the optimal number of features was a bit roundabout, but I have settled on 4 final features to be included in the model because they maximize model precision, recall, and accuracy. The final features chosen represent lags of non-major violations, demerits, number of violations, and critical violations. There is nothing surprising about these features being the most important. Since there are not substantial levels of collinearity perent, I am also confident that these four features are truly the strongest predictors of critical violations. \n",
    "\n",
    "The gain in overall accuracy is only 7% compared to the naive baseline, but hopefully this value can be increased through hyperparameter tuning. It is good that we have higher recall in this scenario, because the city of Las Vegas will want to place a strong emphasis on identifying critical violation restaurants. Recall at 76% means we are catching over 3/4 of critical violators with the model, which is certainly better than the naive baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning & Final Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 20.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1200,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 10,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 46,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Edits to parameters in random forest model\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 1000, stop = 2000, num = 11)]\n",
    "max_features = ['auto',None]\n",
    "max_depth = [int(x) for x in np.linspace(1, 51, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [2, 5, 10]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "params = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = tts(X,y, test_size=.3, random_state= 1)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# Search across 50 iterations and 5 cvs, which totals 250 combos\n",
    "rf = RandomForestClassifier()\n",
    "rf_hyper = RandomizedSearchCV(estimator = rf, param_distributions = params, \n",
    "                               n_iter = 50, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_hyper.fit(X_train, y_train)\n",
    "\n",
    "# Display best parameters\n",
    "rf_hyper.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    0     1\n",
      "Actual              \n",
      "0          951  1068\n",
      "1          608  2094\n",
      "\n",
      "The model precision is: 0.662\n",
      "The model recall is: 0.775\n",
      "The model accuracy is: 0.645\n",
      "\n",
      "\n",
      "Took 10.25 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "# Use best params to get final model efficiency \n",
    "t0 = timeit.default_timer()\n",
    "\n",
    "rf_final = RandomForestClassifier(n_estimators = 1200, min_samples_split = 10, min_samples_leaf = 10,\n",
    "                                 max_features = 'auto', max_depth = 46, bootstrap = True)\n",
    "rf_final.fit(X_train, y_train)\n",
    "pred = rf_final.predict(X_test)\n",
    "pred[pred >= 0.5] = 1\n",
    "pred[pred < 0.5] = 0\n",
    "pred = pred.astype(int)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = pd.crosstab(y_test,pred,rownames=['Actual'],colnames=['Predicted'])\n",
    "print(cm)\n",
    "\n",
    "# Accuracy, Recall, Precision\n",
    "pre = cm.loc[1,1]/(cm.loc[0,1]+cm.loc[1,1])\n",
    "print('\\nThe model precision is: {0}'.format(round(pre,3)))\n",
    "rec = cm.loc[1,1]/(cm.loc[1,0]+cm.loc[1,1])\n",
    "print('The model recall is: {0}'.format(round(rec,3)))\n",
    "acc = (cm.loc[1,1]+cm.loc[0,0])/(cm.loc[1,1]+cm.loc[0,0] + cm.loc[0,1] + cm.loc[1,0])\n",
    "print('The model accuracy is: {0}'.format(round(acc,3)))\n",
    "t1 = timeit.default_timer()\n",
    "total = t1 - t0\n",
    "print('\\n\\nTook {0} seconds to run.'.format(round(total,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After feature selection and hyperparameter training we acheive a recall of 77.5%, which is an upgrade over the naive baseline prediction accuracy of 57.7%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model is not a minimally viable product in its current state due to a few shortcomings. First, the recall of 77.5% improves upon the naive baseline, but I would not be comfortable selling this model to a third party because of the high false negative rate. Several data points would need to be merged into the final dataset in order to add a higher level of richness. I will discuss these data points in detail in the section below.\n",
    "\n",
    "In addition to recall concerns, adding lags into data significantly impacted the scope of restaurants to which we could apply the model. To reiterate, lags were added as main features because an employee working for the City of Las Vegas would only know the previous performance of a restaurant before completing another inspection. Not lagging the data creates a model where the employee is trying to predict a critical violation in time 't', but needs to enter restaurant characterstics from time 't'. The purpose of creating the model is to make critical violations predictable without needing to go to the actual location first. \n",
    "\n",
    "The drawbacks of using lags meant that any restaurant with only one recorded inspection was effectively removed from the dataset. This decision restricted the dataset to ~15,000 observations from ~27,000 observations. So, we can predict critical violations fairly well for restaurants that remain in business over multiple inspections, but have unclear power in predicting violations for new restaurants or those with a single inspection. I would argue that the model will still work for restaurants with one inspection, because their shutdown rates are not out-of-sync with multiple inspection restaurants, which signals some level of homogeneity between the two groups. Violations in new restaurants, on the other hand, are unable to be predicted. I would suggest developing an additional model based on population demographics to address the problem posed by new restaurants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Enhancement (Sky is the limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographic Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Median Income of Consumers at a Restaurant\n",
    "    1. Restaurants may be more inclined to keep their establishment looking nice for customers with higher incomes, regardless of profit margins. Since consumers with higher incomes are able to travel with more ease, their demand is more elastic than the low-income consumer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Crime Rate \n",
    "    1. I have worked on a few other projects with population demographics and crime rate has played a key role. I expect this would be no different with restaurant inspections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Cars per capita in vicinity of restaurant \n",
    "    1. Are many consumers in your region able to drive to your establishment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restaurant Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lease amount ($)\n",
    "    1. Did it get more expensive to operate the restaurant on a monthly basis? This may mean there is less money leftover to keep the restaurant violation-free come inspection time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Real-time Costs and Revenue\n",
    "    1. If we knew that a restaurant was in the red or black prior to the inspection, that would be a clear indication of money available to keep the establishment in good shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Outstanding Debts\n",
    "    1. This would be another measure of a restaurant's financial flexibility. Collinearity with the previous variable could be present, but I would argue there significant variation between real-time profit margin and cumulative debts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Interest Rates paid on loans for each restaurant\n",
    "    1. Did your financial situation just get better or worse? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Minimum Wage (or wages paid by each restaurant) & Wages with tips\n",
    "    1. Another indicator of restaurant status. I imagine minimum-wage restaurants are more delinquent than counterparts who pay competitive wages with higher levels of tips. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Misc. Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Policy Variables\n",
    "    1. Restaurant inspection codes are created based on city/state regulations. If a major policy or regulation was implemented or changed in 2013, then that could dramatically affect inspection decisions. A handful of former critical violations being downgraded in the middle of our sample is something we want to control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Larger Span of Inspections \n",
    "    1. This one is straightforward. The more data points we have over time, the less the dataset is impacted by lags being dropped."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
