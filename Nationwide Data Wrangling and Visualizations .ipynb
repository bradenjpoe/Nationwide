{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling & Visualization Report\n",
    "### Braden Poe | Nationwide Work Sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Overview\n",
    "Build an MVP that allows allows us to **predict** the **likelihood** of a restaurant receiving a **critical inspection**.\n",
    "\n",
    "I envision the final tool being used by the City of Las Vegas to identify restaurants that are at a high risk of critical violations. I am not placing an overwhelming emphasis on computation speed, because I see this product being used to pull reports over a weekend to prioritize the following weeks' inspections. Many of the initial variables in the dataset are not helpful for model training, since they were all collected at the time of an inspection. Creating a model based solely on these variables defeats the purpose of prediction as we would be asking a restaurant inspector to enter information, such as \"number of violations\", that would only be revealed *after* the inspection already took place. To address this problem, I will create lags of all pertinent variables, which will allow information from the previous period to predict violation probabilities in the period of interest. This method will effectively eliminate restaurants with only one inspection from the analysis, but I feel this creates the best environment for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import raw dataset as pandas dataframe\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_excel('rest_data.xlsx')\n",
    "\n",
    "# Print total rows \n",
    "print('Total rows: {0}'.format(len(data)))\n",
    "\n",
    "# Print list of data types\n",
    "print(list(data.dtypes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plenty of objects in the dataset which means a lot of cleaning, dummy generation, and manipulation will need to be done to make this a workable dataset. \n",
    "\n",
    "**Things to consider:**\n",
    "1. Restaurant serial number or permit should be made into an id. Task: find the one that uniquely identifies restauarnts.\n",
    "2. Are there cities in the dataset that should not be there? Task: Make sure all observations are in the LV metro area.\n",
    "3. Task: determine amount of restaurant categories and filter.\n",
    "4. Are there missing observations? Task: Determine if the dataset has missing observations.\n",
    "5. Task: If missing observations exist, either drop them or try to estimate values using a low rank approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List how many observations are in each variable\n",
    "cols = list(data.columns)\n",
    "for i in range(len(data.columns)):\n",
    "    print(str(cols[i]) + ' has this many unique values/strings: {0}'.format(data[cols[i]].nunique()))\n",
    "    \n",
    "data['FIRST_VIOLATION_TYPE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many restaurant categories are there and should we subset?\n",
    "print(data['RESTAURANT_CATEGORY'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a \"primary focus on restaurants\", so I will be subsetting the data to isolate locations that would be classified as a restaurant or something like a restaurant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset data by restaurant type\n",
    "type = list(['Restaurant','Bar / Tavern','Snack Bar','Buffet'])\n",
    "data.sort_values(by=['RESTAURANT_CATEGORY'],inplace=True)\n",
    "rsub = data.loc[(data['RESTAURANT_CATEGORY']== type[0]) | (data['RESTAURANT_CATEGORY']== type[1]) | (data['RESTAURANT_CATEGORY']== type[2]) | (data['RESTAURANT_CATEGORY']== type[3])]\n",
    "\n",
    "# Now that we have a nice subset of the data, let's begin to look for duplicate entries.\n",
    "dup = rsub.duplicated()\n",
    "dup.replace(False,0,inplace=True)\n",
    "count = dup.sum()\n",
    "print('The dataset has {0} duplicate entries.'.format(count))\n",
    "\n",
    "# Do locations seem to be duplicated? \n",
    "dup = rsub['LAT_LONG_RAW'].nunique()\n",
    "print('The dataset has {0} unique locations.'.format(dup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are 26700 observations in the subset, there are only 5344 unique locations. This tells us that many restaurants or bars have multiple components within the same location that all require separate inspections, but contribute to the function of a single business. Example: The Flamingo Hotel has multiple pockets of kitchens, bars, and restaurants all located in the same latitude and longitude. Presumably, these pockets all help one overarching restaurant operate. Since I cannot definitively say this is true, I will not be modifying the dataset, but will keep it in mind when discussing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permit number describes an individual restaurant while serial number describes the inspection of a restaurant\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rsub.sort_values(by=['RESTAURANT_PERMIT_NUMBER','INSPECTION_DATE'],inplace=True)\n",
    "\n",
    "# Create reinspection dummy\n",
    "rsub['re_dummy'] = rsub['INSPECTION_TYPE']\n",
    "rsub['re_dummy'] = rsub['re_dummy'].replace(['Routine Inspection','Re-inspection'],[0,1])\n",
    "\n",
    "# Let's groupby permit number and create a lag variable for re-inspection. \n",
    "# This lag tells us if a re-inspection occured prior to the current inspection\n",
    "rsub['re_lag'] = rsub.groupby('RESTAURANT_PERMIT_NUMBER')['re_dummy'].shift(1)\n",
    "\n",
    "## Drop Re-inspection \n",
    "rsub = rsub[rsub['INSPECTION_TYPE'] != 'Re-inspection']\n",
    "\n",
    "# Create lag for number of violations\n",
    "rsub['numvio_lag'] = rsub.groupby('RESTAURANT_PERMIT_NUMBER')['NUMBER_OF_VIOLATIONS'].shift(1)\n",
    "\n",
    "# Number of critical violations \n",
    "def rule1(x,y,z):\n",
    "    if x == 'Critical' and y == 'Critical' and z == 'Critical':\n",
    "        n = 3\n",
    "    elif (x == 'Critical' and y == 'Critical') or (x == 'Critical' and z == 'Critical') or (z == 'Critical' and y == 'Critical'):\n",
    "        n = 2\n",
    "    elif x == 'Critical' or y == 'Critical' or z == 'Critical':\n",
    "        n = 1\n",
    "    else: \n",
    "        n = 0\n",
    "    return n\n",
    "\n",
    "# Number of Major violations\n",
    "def rule2(x,y,z):\n",
    "    if x == 'Major' and y == 'Major' and z == 'Major':\n",
    "        n = 3\n",
    "    elif (x == 'Major' and y == 'Major') or (x == 'Major' and z == 'Major') or (z == 'Major' and y == 'Major'):\n",
    "        n = 2\n",
    "    elif x == 'Major' or y == 'Major' or z == 'Major':\n",
    "        n = 1\n",
    "    else: \n",
    "        n = 0\n",
    "    return n\n",
    "\n",
    "# Number of Non-major violations\n",
    "def rule3(x,y,z):\n",
    "    if x == 'Non-Major' and y == 'Non-Major' and z == 'Non-Major':\n",
    "        n = 3\n",
    "    elif (x == 'Non-Major' and y == 'Non-Major') or (x == 'Non-Major' and z == 'Non-Major') or (z == 'Non-Major' and y == 'Non-Major'):\n",
    "        n = 2\n",
    "    elif x == 'Non-Major' or y == 'Non-Major' or z == 'Non-Major':\n",
    "        n = 1\n",
    "    else: \n",
    "        n = 0\n",
    "    return n\n",
    "\n",
    "# Number of imminent health hazards\n",
    "def rule4(x,y,z):\n",
    "    t = 'Imminent Health Hazard'\n",
    "    if x == t and y == t and z == t:\n",
    "        n = 3\n",
    "    elif (x == t and y == t) or (x == t and z == t) or (z == t and y == t):\n",
    "        n = 2\n",
    "    elif x == t or y == t or z == t:\n",
    "        n = 1\n",
    "    else: \n",
    "        n = 0\n",
    "    return n\n",
    "    \n",
    "rsub['crit_ct'] = rsub.apply(lambda x: rule1(x['FIRST_VIOLATION_TYPE'],x['SECOND_VIOLATION_TYPE'],x['THIRD_VIOLATION_TYPE']),axis=1).astype(int)\n",
    "rsub['major_ct'] = rsub.apply(lambda x: rule2(x['FIRST_VIOLATION_TYPE'],x['SECOND_VIOLATION_TYPE'],x['THIRD_VIOLATION_TYPE']),axis=1)\n",
    "rsub['nm_ct'] = rsub.apply(lambda x: rule3(x['FIRST_VIOLATION_TYPE'],x['SECOND_VIOLATION_TYPE'],x['THIRD_VIOLATION_TYPE']),axis=1)\n",
    "rsub['other_ct'] = rsub.apply(lambda x: rule4(x['FIRST_VIOLATION_TYPE'],x['SECOND_VIOLATION_TYPE'],x['THIRD_VIOLATION_TYPE']),axis=1)\n",
    "\n",
    "\n",
    "# Create lags of violation type \n",
    "rsub['crit_lag'] = rsub.groupby('RESTAURANT_PERMIT_NUMBER')['crit_ct'].shift(1)\n",
    "rsub['major_lag'] = rsub.groupby('RESTAURANT_PERMIT_NUMBER')['major_ct'].shift(1)\n",
    "rsub['nm_lag'] = rsub.groupby('RESTAURANT_PERMIT_NUMBER')['nm_ct'].shift(1)\n",
    "rsub['other_lag'] = rsub.groupby('RESTAURANT_PERMIT_NUMBER')['other_ct'].shift(1)\n",
    "\n",
    "# Create lags of inspection demerits \n",
    "rsub['demerits_lag'] = rsub.groupby('RESTAURANT_PERMIT_NUMBER')['INSPECTION_DEMERITS'].shift(1)\n",
    "\n",
    "# Create grade lag \n",
    "rsub['grade_lag'] = rsub.groupby('RESTAURANT_PERMIT_NUMBER')['INSPECTION_GRADE'].shift(1)\n",
    "\n",
    "# Create first violation type = critical as a response \n",
    "rsub['response'] = (rsub['FIRST_VIOLATION_TYPE'] == 'Critical').astype(int)\n",
    "\n",
    "# Clean up city name errors\n",
    "rsub['CITY'].loc[rsub['CITY']=='HendeSON'] = 'Henderson'\n",
    "rsub['CITY'].loc[rsub['CITY']=='HENDERSON'] = 'Henderson'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the issues with this analysis is that we're looking to build a product that can help predict restaurant inspections, but we don't have much of a time dimension in the current dataset. This means that most of the inputs into the model will not be known before an inspection. The creation of lags is an attempt to address this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up rsub into the final set we will use\n",
    "final = rsub\n",
    "\n",
    "## Recategorize by restaurant type \n",
    "final['RESTAURANT_CATEGORY'].loc[final['RESTAURANT_CATEGORY']=='Restaurant'] = 0\n",
    "final['RESTAURANT_CATEGORY'].loc[final['RESTAURANT_CATEGORY']=='Bar / Tavern'] = 1\n",
    "final['RESTAURANT_CATEGORY'].loc[final['RESTAURANT_CATEGORY']=='Buffet'] = 2\n",
    "final['RESTAURANT_CATEGORY'].loc[final['RESTAURANT_CATEGORY']=='Snack Bar'] = 3\n",
    "\n",
    "# Trim zip since there are too many unique zip codes to be included in the analysis\n",
    "## Drop missing zips \n",
    "final['ZIP'] = final['ZIP'].dropna()\n",
    "final['ziptrim'] = final['ZIP'].str.slice(start = 0, stop = 5, step =1)\n",
    "\n",
    "# Add in grade indicator\n",
    "final['INSPECTION_GRADE'] = final['INSPECTION_GRADE'].dropna()\n",
    "final['INSPECTION_GRADE'] = final['INSPECTION_GRADE'].astype(str)\n",
    "final['INSPECTION_GRADE'] = final['INSPECTION_GRADE'].loc[final['INSPECTION_GRADE']!='nan']\n",
    "final['INSPECTION_GRADE'].loc[final['INSPECTION_GRADE']=='a'] = 'A'\n",
    "final['A'] = (final['INSPECTION_GRADE']=='A').astype(int)\n",
    "final['B'] = (final['INSPECTION_GRADE']=='B').astype(int)\n",
    "final['C'] = (final['INSPECTION_GRADE']=='C').astype(int)\n",
    "final['O'] = (final['INSPECTION_GRADE']=='O').astype(int)\n",
    "final['X'] = (final['INSPECTION_GRADE']=='X').astype(int)\n",
    "\n",
    "# Add in lags of grades \n",
    "# Create lags of violation type \n",
    "final['a_lag'] = final['A'].shift(1)\n",
    "final['b_lag'] = final['B'].shift(1)\n",
    "final['c_lag'] = final['C'].shift(1)\n",
    "final['o_lag'] = final['O'].shift(1)\n",
    "final['x_lag'] = final['X'].shift(1)\n",
    "\n",
    "# Top violations may be significant in prediciting critical vs non-critical, so let's make dummies of the top 10.\n",
    "final['FIRST_VIOLATION'] = final['FIRST_VIOLATION'].astype(str)\n",
    "dums = pd.get_dummies(final['FIRST_VIOLATION'])\n",
    "keep = ['202','209','211','214','206','14','13','212','213','208']\n",
    "dums= dums[keep]\n",
    "final = pd.concat([final,dums],axis=1)\n",
    "# Lag first violations\n",
    "final[keep] = final[keep].shift(1)\n",
    "\n",
    "#Drop unwanted columns\n",
    "strip = ['RESTAURANT_SERIAL_NUMBER','RESTAURANT_NAME','RESTAURANT_LOCATION','STATE','CURRENT_GRADE',\n",
    "        'DATE_CURRENT','INSPECTION_DATE_RAW','INSPECTION_DATE','CURRENT_DEMERITS','DATE_CURRENT',\n",
    "        'INSPECTION_TYPE','PERMIT_STATUS','INSPECTION_RESULT','RECORD_UPDATED','LAT_LONG_RAW','FIRST_VIOLATION_DS',\n",
    "        'SECOND_VIOLATION_DS','THIRD_VIOLATION_DS','INSPECTION_TIME','EMPLOYEE_ID','VIOLATIONS_RAW']\n",
    "final.drop(strip,axis=1,inplace=True)\n",
    "\n",
    "# Rename columns for ease\n",
    "final.rename(columns={'RESTAURANT_PERMIT_NUMBER':'permit','RESTAURANT_CATEGORY':'category','INSPECTION_DEMERITS':'demerits',\n",
    "                     'INSPECTION_GRADE':'grade','LOCATION_LATITUDE':'lat','LOCATION_LONGITUDE':'long',\n",
    "                     'FIRST_VIOLATION':'first','SECOND_VIOLATION':'second','THIRD_VIOLATION':'third',\n",
    "                     'FIRST_VIOLATION_TYPE':'firsttype','SECOND_VIOLATION_TYPE':'secondtype',\n",
    "                     'THIRD_VIOLATION_TYPE':'thirdtype','NUMBER_OF_VIOLATIONS':'numvio'},inplace=True)\n",
    "final.columns = final.columns.str.lower()\n",
    "\n",
    "# Create a count column that will come in handy during aggregation for visualizations\n",
    "final['count'] = 1\n",
    "\n",
    "# Get trimmed zip code dummies\n",
    "dums = pd.get_dummies(final['ziptrim'])\n",
    "final = pd.concat([final,dums],axis=1)\n",
    "\n",
    "# Create city dummies\n",
    "final['city'] = final['city'].astype(str)\n",
    "final['city'] = final['city'].str.lower()\n",
    "dums = pd.get_dummies(final['city'])\n",
    "final = pd.concat([final,dums],axis=1)\n",
    "final = final.drop('nan',axis=1)\n",
    "\n",
    "# View final set of variables\n",
    "x = np.asarray(final.columns)\n",
    "with np.printoptions(threshold=np.inf):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables look all set. I will worry about standardizing in the prediction section of the report. I also want to keep in mind the somewhat high dimensionality of the dataset - will look to use an aggressive regularizer in feature selection since all of these dummies are not significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the data allows us to view variability within the dataset in several different ways. I will generate heatmaps using Geopandas to represent the density of violations by zipcode weighted by the number of inspections per zip code. These heatmaps will provide insight into critical violation hotspots in the state and will help us determine if it would be helpful to use location in our predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shapefile of LV zip codes \n",
    "import geopandas as gpd\n",
    "\n",
    "LV = 'Zip_Codes.shp'\n",
    "map = gpd.read_file(LV)\n",
    "\n",
    "# Aggregate the data on the zipcode level so that we can merge it with the shapefile\n",
    "agg = final.groupby('ziptrim').agg({'crit_ct':'sum','major_ct':'sum','nm_ct':'sum','count':'sum','demerits':'sum'})\n",
    "agg['crit_ct'] = agg['crit_ct'].astype(int)\n",
    "agg['major_ct'] = agg['major_ct'].astype(int)\n",
    "agg['nm_ct'] = agg['nm_ct'].astype(int)\n",
    "agg['demerits'] = agg['demerits'].astype(int)\n",
    "agg = agg.reset_index()\n",
    "agg['ziptrim'] = agg['ziptrim'].astype(int)\n",
    "\n",
    "# Per inspection weighting because we don't want # of inspections to skew violation count\n",
    "agg['crit_per'] = agg['crit_ct']/agg['count']\n",
    "agg['major_per'] = agg['major_ct']/agg['count']\n",
    "agg['nm_per'] = agg['nm_ct']/agg['count']\n",
    "\n",
    "# Per inspection demerits\n",
    "agg['dem_per'] = agg['demerits']/agg['count']\n",
    "\n",
    "# Merging aggregated data with the shapefile\n",
    "merged = map.set_index('ZIP').join(agg.set_index('ziptrim'))\n",
    "merged['crit_per'] = merged['crit_per'].fillna(0)\n",
    "merged['major_per'] = merged['major_per'].fillna(0)\n",
    "merged['nm_per'] = merged['nm_per'].fillna(0)\n",
    "merged['dem_per'] = merged['dem_per'].fillna(0)\n",
    "\n",
    "# Generating plot\n",
    "# set the range for the choropleth\n",
    "vmin, vmax = 0.0, 2.5\n",
    "# create figure and axes for Matplotlib\n",
    "fig, ax = plt.subplots(1,3, figsize=(30, 10))\n",
    "merged.plot(column='crit_per', cmap='Blues', linewidth=0.8, ax=ax[0], edgecolor='0.8')\n",
    "merged.plot(column='major_per', cmap='Oranges', linewidth=0.8, ax=ax[1], edgecolor='0.8')\n",
    "merged.plot(column='nm_per', cmap='Greens', linewidth=0.8, ax=ax[2], edgecolor='0.8')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[0].set_title('Critical Violations per Inspection by Zip', fontdict={'fontsize': '25', 'fontweight' : '3'})\n",
    "\n",
    "\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[1].set_title('Major Violations per Inspection by Zip', fontdict={'fontsize': '25', 'fontweight' : '3'})\n",
    "\n",
    "ax[1].annotate('Source: City of Las Vegas Open Data, 2019',xy=(0.1, .08),\n",
    "            xycoords='figure fraction', horizontalalignment='left',\n",
    "            verticalalignment='top', fontsize=12, color='#555555')\n",
    "\n",
    "ax[2].axis('off')\n",
    "\n",
    "ax[2].set_title('Non-Major Violations per Inspection by Zip', fontdict={'fontsize': '25', 'fontweight' : '3'})\n",
    "\n",
    "\n",
    "\n",
    "fig.savefig('critical.png', dpi=300)\n",
    "plt.close(fig='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional heatmap:  \n",
    "The heatmaps created above do not look at demerits per inspection, but I will also investigate this variable to see if it differs from critical violations per inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "# set the range for the choropleth\n",
    "vmin, vmax = 0.0, merged['dem_per'].max()\n",
    "\n",
    "# create figure and axes for Matplotlib\n",
    "fig, ax = plt.subplots(1,2, figsize=(15, 5))\n",
    "merged.plot(column='dem_per', cmap='Blues', linewidth=0.8, ax=ax[0], edgecolor='0.8')\n",
    "merged.plot(column='crit_per',cmap='Blues',linewidth=0.8, ax=ax[1], edgecolor='0.8')\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[0].set_title('Demerits per Inspection by Zip', fontdict={'fontsize': '15', 'fontweight' : '3'})\n",
    "ax[1].set_title('Critical Violations per Inspection by Zip',fontdict={'fontsize': '15', 'fontweight' : '3'})\n",
    "\n",
    "fig.savefig('dem_vs_critical.png', dpi=300)\n",
    "plt.close(fig='all')\n",
    "\n",
    "# Generate variables to call heatmaps in discussion below.\n",
    "from PIL import Image as i\n",
    "crit = i.open('critical.png')\n",
    "dem = i.open('dem_vs_critical.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap 1:\n",
    "Listed below is the first set of heatmaps, which compares violation types between zip codes. Because the Las Vegas Metro area is so large, the heatmaps are spanning a broad area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmaps are helpful in gaining a better understanding of where violations are occuring and at what frequency. The maps look at violations per inspection and I only consider the first three listed violations. Here are a few immediate takeaways that I will keep in mind during the prediction portion of the report.\n",
    "1. The types of violations we see are not constantly distributed across the state. \n",
    "2. More densely populated zip codes have a higher concentration of critical violations than less densely populated regions. \n",
    "3. Judging by the density of critical violations, I imagine that location is going to play an extremely strong role in predicting critical violations. I want to ensure there is enough variability in the dataset, because our MVP won't be helpful if it is simply predicting violation probability in period \"t\" based on location and violation status in period \"t-1\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second heatmap listed below compares demerits per inspection with critical violations per inspection. The difference in magnitudes does not matter between these two graphs as I am more concerned with correlations between zip codes. The closer the colors means the higher the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does appear to be some variability between demerits per inspection and critical violations per inspection at the zip code level, which indicates that collinearity should not be a significant problem the models we run as long as location is controlled for in some manner. That being said, I will still press forward with a correlation matrix and statistical tests to rule out concern for multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seaborn to create matrix \n",
    "import seaborn as sns\n",
    "\n",
    "# Restrict final to variables of interest.\n",
    "cols_to_get = ['response','numvio_lag','re_lag','crit_lag','major_lag','nm_lag','other_lag','demerits_lag',\n",
    "               'a_lag','b_lag','c_lag','o_lag','x_lag']\n",
    "\n",
    "df = final[cols_to_get]\n",
    "corrMatrix = df.corr(method=\"pearson\").round(2)\n",
    "x = corrMatrix['response'].abs().sort_values(ascending=False)\n",
    "f, ax = plt.subplots(figsize=(10, 7))\n",
    "sns.heatmap(corrMatrix, annot=True,ax=ax)\n",
    "plt.title('Pearson Correlation Matrix', fontsize=25,pad=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume a non-linear relationship between variables\n",
    "corrMatrix = df.corr(method=\"spearman\").round(2)\n",
    "x = corrMatrix['response'].abs().sort_values(ascending=False)\n",
    "f, ax = plt.subplots(figsize=(10, 7))\n",
    "sns.heatmap(corrMatrix, annot=True,ax=ax)\n",
    "plt.title('Spearman Correlation Matrix', fontsize=25,pad=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I don't want to perform feature selection yet, this correlation matrix paints a clearer picture of important variables. In addition to the importance of location, I have ranked the following variables below from most correlated with response to least correlated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spearman\n",
    "corrMatrix['response'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The immediate takeaways are that poor performance in the previous period is highly correlated with poor performance in the current period. Higher numbers of demerits, total violations, and critical violations all lead to a greater chance of a critical violation in the current period. Non-major violations in the previous period has a correlation that is exactly the opposite of critical violations and an A-grade in the previous period is also negatively correlated with response.\n",
    "\n",
    "I'm fairly happy with the overall output of both correlation matrices. Whether we make distributional assumptions using Pearson's correlation coefficient or allow non-linearity with Spearman's coefficient, it is clear that most variables are *not* highly correlated with one another. This conclusion will be helpful to remember when interpreting random forest output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Plots Code: Critical vs. Non-Critical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a critical subset \n",
    "crit = final.loc[final['response']==1]\n",
    "\n",
    "# Make a non-crit subset\n",
    "noncrit = final.loc[final['response']==0]\n",
    "\n",
    "# Create distributonal subplots for non-location based data\n",
    "fig, ax = plt.subplots(2,2,figsize=(15,10))  \n",
    "plt.subplots_adjust(hspace=.3)\n",
    "# Demerits_Lag distribution plot\n",
    "sns.distplot(crit['demerits_lag'], hist = False, kde = True,\n",
    "                 kde_kws = {'shade': True, 'linewidth': 3}, label = 'Crit',ax=ax[0,0])\n",
    "sns.distplot(noncrit['demerits_lag'], hist = False, kde = True,\n",
    "                 kde_kws = {'shade': True, 'linewidth': 3}, label = 'Non-Crit',ax=ax[0,0])\n",
    "ax[0,0].set_xlabel('Lagged Demerits')\n",
    "ax[0,0].set_ylabel('Density')\n",
    "ax[0,0].set_title('Density Distribution of Lagged Demerits')\n",
    "\n",
    "#Number of Violations lagged distribution plot\n",
    "sns.distplot(crit['numvio_lag'], hist = False, kde = True,\n",
    "                 kde_kws = {'shade': True, 'linewidth': 3}, label = 'Crit',ax=ax[0,1])\n",
    "sns.distplot(noncrit['numvio_lag'], hist = False, kde = True,\n",
    "                 kde_kws = {'shade': True, 'linewidth': 3}, label = 'Non-Crit',ax=ax[0,1])\n",
    "ax[0,1].set_xlabel('Lagged Violations')\n",
    "ax[0,1].set_ylabel('Density')\n",
    "ax[0,1].set_title('Density Distribution of Lagged Violations')\n",
    "\n",
    "# Lagged Critical Violation Count\n",
    "ax[1,0].hist([crit['crit_lag'],noncrit['crit_lag']],label=['Crit','Non-Crit'])\n",
    "ax[1,0].legend()\n",
    "ax[1,0].set_title('Histogram of Lagged Critical Violations')\n",
    "ax[1,0].set_xlabel('Lagged Critical Violations')\n",
    "ax[1,0].set_ylabel('Count')\n",
    "\n",
    "# Lagged Non-major violation count\n",
    "ax[1,1].hist([crit['nm_lag'],noncrit['nm_lag']],label=['Crit','Non-Crit'])\n",
    "ax[1,1].legend()\n",
    "ax[1,1].set_title('Histogram of Lagged Non-Major Violations')\n",
    "ax[1,1].set_xlabel('Lagged Non-Major Violations')\n",
    "ax[1,1].set_ylabel('Count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Plots Analysis: Critical vs. Non-Critical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots are a nice visual confirmation that the variables have different distributions for both sides of the binary response, but the data is also highly inseparable. This is an indication that any type of SVM may have difficulty creating an adequate margin for the following prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several key takeaways that I want to highlight before moving into the predictive portion of the report. \n",
    "1. First, predicted critical violation status appears to rely heavily on a restaurants past performance, which indicates that restaurants exhibit somewhat static behavior over time.\n",
    "2. Location will play a key factor in predicted violation based on the heat maps generated in this report. I expect that many of the city and zip code indicator variables will remain after regularization in the predictive stage.\n",
    "3. Many observations needed to be dropped in order to make analysis statistically viable. As a result, the predictive model will likely perform worse on new restaurants, because restaurants with only one inspection are removed from the training data.\n",
    "4. The inseparability of the dependent variable means that a support vector machine is not the optimal approach to completing this prediction task.\n",
    "5. Creating more accurate predictions relies heavily on merging additional data into the model, which will be discussed at the end of the following report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Trimming of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any variables that I am 100% certain won't be used in the analysis:\n",
    "with np.printoptions(threshold=np.inf):\n",
    "    print(np.array(crit.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns \n",
    "\n",
    "cols_to_drop = ['permit','category','address','city','zip','demerits','grade','lat','long',\n",
    "               'first','second','third','firsttype','secondtype','thirdtype','numvio','re_dummy',\n",
    "               're_lag','ziptrim','count','grade_lag']\n",
    "\n",
    "final = final.drop(cols_to_drop,axis=1)\n",
    "\n",
    "# Drop missing values that are a result of the lags \n",
    "final = final.dropna(axis=0)\n",
    "\n",
    "final.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no missing values remaining we are perfectly set up to run some predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('predict.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
